{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/czfhmzps76q12__f89qmdcy00000gn/T/ipykernel_25938/4187335456.py:49: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No more results found after 0 papers. Fetching complete!\n",
      "\n",
      "✅ Completed! Collected 0 AI research papers and saved them to 'all_ai_research_papers.csv'.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Get the current year and compute the last 10 years\n",
    "current_year = datetime.datetime.now().year\n",
    "start_year = current_year - 10  # Get the year from 10 years ago\n",
    "start_date = f\"{start_year}0101\"  # Format: YYYYMMDD\n",
    "end_date = f\"{current_year}1231\"  # Format: YYYYMMDD\n",
    "\n",
    "# Define search query with a date range for the last 10 years\n",
    "query = f\"(artificial intelligence OR deep learning OR machine learning OR transformers) AND submittedDate:[{start_date} TO {end_date}]\"\n",
    "\n",
    "# Initialize result list\n",
    "papers = []\n",
    "\n",
    "# Pagination settings\n",
    "max_results_per_query = 100  # arXiv allows up to 100 results per request\n",
    "total_results = 500  # Adjust as needed\n",
    "start_index = 0  # Start from first result\n",
    "\n",
    "while start_index < total_results:\n",
    "    try:\n",
    "        # Fetch papers\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results_per_query,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        # Process results\n",
    "        batch_papers = []\n",
    "        for result in search.results():\n",
    "            batch_papers.append({\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"published_date\": result.published.date(),\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"arxiv_url\": result.entry_id\n",
    "            })\n",
    "\n",
    "        # Check if results are empty\n",
    "        if not batch_papers:\n",
    "            print(f\"⚠️ No results found for start index {start_index}. Skipping.\")\n",
    "            break  # Exit loop if no results are returned\n",
    "\n",
    "        papers.extend(batch_papers)\n",
    "        print(f\"✅ Retrieved {len(batch_papers)} papers (Start index: {start_index})\")\n",
    "\n",
    "        # Move to the next batch\n",
    "        start_index += max_results_per_query\n",
    "        time.sleep(2)  # Avoid hitting API rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error at start index {start_index}: {e}\")\n",
    "        print(\"🔄 Retrying after a short delay...\")\n",
    "        time.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(papers)\n",
    "df.to_csv(\"ai_research_papers_last_10_years.csv\", index=False)\n",
    "\n",
    "print(f\"\\n✅ Completed! Collected {len(df)} AI research papers from {start_year} to {current_year}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>published_date</th>\n",
       "      <th>categories</th>\n",
       "      <th>arxiv_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyzing Interference from Static Cellular Co...</td>\n",
       "      <td>The problem of base station cooperation has re...</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>cs.IT, math.IT</td>\n",
       "      <td>http://arxiv.org/abs/1502.00033v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHOE: Supervised Hashing with Output Embeddings</td>\n",
       "      <td>We present a supervised binary encoding scheme...</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>http://arxiv.org/abs/1502.00030v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unconventional phase selection in high-driven ...</td>\n",
       "      <td>Phase selection in deeply undercooled liquids ...</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>cond-mat.mtrl-sci</td>\n",
       "      <td>http://arxiv.org/abs/1502.00023v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Characterizing Transiting Planet Atmospheres t...</td>\n",
       "      <td>[Abridged] We have only been able to comprehen...</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>astro-ph.EP</td>\n",
       "      <td>http://arxiv.org/abs/1502.00004v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Threshold corrections, generalised prepotentia...</td>\n",
       "      <td>We continue our study of one-loop integrals as...</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>hep-th, math.NT</td>\n",
       "      <td>http://arxiv.org/abs/1502.00007v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Analyzing Interference from Static Cellular Co...   \n",
       "1    SHOE: Supervised Hashing with Output Embeddings   \n",
       "2  Unconventional phase selection in high-driven ...   \n",
       "3  Characterizing Transiting Planet Atmospheres t...   \n",
       "4  Threshold corrections, generalised prepotentia...   \n",
       "\n",
       "                                            abstract published_date  \\\n",
       "0  The problem of base station cooperation has re...     2015-01-30   \n",
       "1  We present a supervised binary encoding scheme...     2015-01-30   \n",
       "2  Phase selection in deeply undercooled liquids ...     2015-01-30   \n",
       "3  [Abridged] We have only been able to comprehen...     2015-01-30   \n",
       "4  We continue our study of one-loop integrals as...     2015-01-30   \n",
       "\n",
       "          categories                          arxiv_url  \n",
       "0     cs.IT, math.IT  http://arxiv.org/abs/1502.00033v1  \n",
       "1              cs.CV  http://arxiv.org/abs/1502.00030v1  \n",
       "2  cond-mat.mtrl-sci  http://arxiv.org/abs/1502.00023v1  \n",
       "3        astro-ph.EP  http://arxiv.org/abs/1502.00004v1  \n",
       "4    hep-th, math.NT  http://arxiv.org/abs/1502.00007v2  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 100 papers (Total: 100)\n",
      "\n",
      "✅ Completed! Collected 100 AI research papers and saved them to 'all_ai_research_papers.csv'.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the AI research query\n",
    "query = \"cat:cs.AI AND (artificial intelligence OR deep learning OR machine learning OR neural networks OR supervised learning OR unsupervised learning OR semi-supervised learning OR reinforcement learning OR self-supervised learning OR few-shot learning OR zero-shot learning OR contrastive learning OR multi-modal learning OR transfer learning OR generative models OR autoencoders OR diffusion models OR attention mechanisms OR representation learning OR adversarial learning OR optimization algorithms OR neurosymbolic AI OR causality in AI OR graph-based learning OR computational neuroscience OR meta-learning OR scalable AI OR knowledge distillation OR neural compression OR transformers OR large language models OR prompt engineering OR in-context learning OR LLMs OR GPT OR BERT OR T5 OR vision transformers OR graph neural networks OR diffusion models OR GANs OR VAE OR AI fairness OR explainable AI OR federated learning OR privacy-preserving AI OR quantum machine learning OR neuromorphic computing OR AI ethics OR human-AI collaboration OR robotics and AI OR autonomous vehicles OR AI for healthcare OR AI for finance OR AI for scientific discovery OR AI for cybersecurity OR adversarial robustness OR AI for business forecasting OR AI for industrial automation OR AI for climate change OR AI for social good OR trustworthy AI OR causal inference OR explainable reinforcement learning) AND submittedDate:[20150101 TO 20251231]\"\n",
    "\n",
    "# Initialize result storage\n",
    "papers = []\n",
    "batch_size = 100  # Maximum allowed per request\n",
    "file_path = \"all_ai_research_papers.csv\"\n",
    "\n",
    "# Initialize the arXiv API client\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_to_csv(data, file_path, first_save=False):\n",
    "    df = pd.DataFrame(data)\n",
    "    mode = 'w' if first_save else 'a'  # 'w' creates a new file, 'a' appends data\n",
    "    header = first_save  # Only include headers if creating a new file\n",
    "    df.to_csv(file_path, mode=mode, header=header, index=False)\n",
    "\n",
    "# Fetch papers continuously until no more are available\n",
    "first_save = True  # Flag to track first save operation\n",
    "\n",
    "try:\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=batch_size,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    # Fetch results using the updated method\n",
    "    batch_papers = []\n",
    "    for result in client.results(search):  # FIX: Use `Client.results()`\n",
    "        batch_papers.append({\n",
    "            \"title\": result.title,\n",
    "            \"abstract\": result.summary,\n",
    "            \"published_date\": result.published.date(),\n",
    "            \"categories\": \", \".join(result.categories),\n",
    "            \"arxiv_url\": result.entry_id\n",
    "        })\n",
    "\n",
    "    # Save batch to CSV\n",
    "    if batch_papers:\n",
    "        save_to_csv(batch_papers, file_path, first_save)\n",
    "        papers.extend(batch_papers)\n",
    "        print(f\"✅ Retrieved {len(batch_papers)} papers (Total: {len(papers)})\")\n",
    "    else:\n",
    "        print(f\"✅ No more results found. Fetching complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred: {e}\")\n",
    "    print(\"🔄 Retrying after a short delay...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"\\n✅ Completed! Collected {len(papers)} AI research papers and saved them to '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/66/czfhmzps76q12__f89qmdcy00000gn/T/ipykernel_25938/2010317569.py:32: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 100 papers (Total: 100)\n",
      "✅ Retrieved 100 papers (Total: 200)\n",
      "✅ Retrieved 100 papers (Total: 300)\n",
      "✅ Retrieved 100 papers (Total: 400)\n",
      "✅ Retrieved 100 papers (Total: 500)\n",
      "✅ Retrieved 100 papers (Total: 600)\n",
      "✅ Retrieved 100 papers (Total: 700)\n",
      "✅ Retrieved 100 papers (Total: 800)\n",
      "✅ Retrieved 100 papers (Total: 900)\n",
      "✅ Retrieved 100 papers (Total: 1000)\n",
      "✅ Retrieved 100 papers (Total: 1100)\n",
      "✅ Retrieved 100 papers (Total: 1200)\n",
      "✅ Retrieved 100 papers (Total: 1300)\n",
      "✅ Retrieved 100 papers (Total: 1400)\n",
      "✅ Retrieved 100 papers (Total: 1500)\n",
      "✅ Retrieved 100 papers (Total: 1600)\n",
      "✅ Retrieved 100 papers (Total: 1700)\n",
      "✅ Retrieved 100 papers (Total: 1800)\n",
      "✅ Retrieved 100 papers (Total: 1900)\n",
      "✅ Retrieved 100 papers (Total: 2000)\n",
      "✅ Retrieved 100 papers (Total: 2100)\n",
      "✅ Retrieved 100 papers (Total: 2200)\n",
      "✅ Retrieved 100 papers (Total: 2300)\n",
      "✅ Retrieved 100 papers (Total: 2400)\n",
      "✅ Retrieved 100 papers (Total: 2500)\n",
      "✅ Retrieved 100 papers (Total: 2600)\n",
      "✅ Retrieved 100 papers (Total: 2700)\n",
      "✅ Retrieved 100 papers (Total: 2800)\n",
      "✅ Retrieved 100 papers (Total: 2900)\n",
      "✅ Retrieved 100 papers (Total: 3000)\n",
      "✅ Retrieved 100 papers (Total: 3100)\n",
      "✅ Retrieved 100 papers (Total: 3200)\n",
      "✅ Retrieved 100 papers (Total: 3300)\n",
      "✅ Retrieved 100 papers (Total: 3400)\n",
      "✅ Retrieved 100 papers (Total: 3500)\n",
      "✅ Retrieved 100 papers (Total: 3600)\n",
      "✅ Retrieved 100 papers (Total: 3700)\n",
      "✅ Retrieved 100 papers (Total: 3800)\n",
      "✅ Retrieved 100 papers (Total: 3900)\n",
      "✅ Retrieved 100 papers (Total: 4000)\n",
      "✅ Retrieved 100 papers (Total: 4100)\n",
      "✅ Retrieved 100 papers (Total: 4200)\n",
      "✅ Retrieved 100 papers (Total: 4300)\n",
      "✅ Retrieved 100 papers (Total: 4400)\n",
      "✅ Retrieved 100 papers (Total: 4500)\n",
      "✅ Retrieved 100 papers (Total: 4600)\n",
      "✅ Retrieved 100 papers (Total: 4700)\n",
      "✅ Retrieved 100 papers (Total: 4800)\n",
      "✅ Retrieved 100 papers (Total: 4900)\n",
      "✅ Retrieved 100 papers (Total: 5000)\n",
      "✅ Retrieved 100 papers (Total: 5100)\n",
      "✅ Retrieved 100 papers (Total: 5200)\n",
      "✅ Retrieved 100 papers (Total: 5300)\n",
      "✅ Retrieved 100 papers (Total: 5400)\n",
      "✅ Retrieved 100 papers (Total: 5500)\n",
      "✅ Retrieved 100 papers (Total: 5600)\n",
      "✅ Retrieved 100 papers (Total: 5700)\n",
      "✅ Retrieved 100 papers (Total: 5800)\n",
      "✅ Retrieved 100 papers (Total: 5900)\n",
      "✅ Retrieved 100 papers (Total: 6000)\n",
      "✅ Retrieved 100 papers (Total: 6100)\n",
      "✅ Retrieved 100 papers (Total: 6200)\n",
      "✅ Retrieved 100 papers (Total: 6300)\n",
      "✅ Retrieved 100 papers (Total: 6400)\n",
      "✅ Retrieved 100 papers (Total: 6500)\n",
      "✅ Retrieved 100 papers (Total: 6600)\n",
      "✅ Retrieved 100 papers (Total: 6700)\n",
      "✅ Retrieved 100 papers (Total: 6800)\n",
      "✅ Retrieved 100 papers (Total: 6900)\n",
      "✅ Retrieved 100 papers (Total: 7000)\n",
      "✅ Retrieved 100 papers (Total: 7100)\n",
      "✅ Retrieved 100 papers (Total: 7200)\n",
      "✅ Retrieved 100 papers (Total: 7300)\n",
      "✅ Retrieved 100 papers (Total: 7400)\n",
      "✅ Retrieved 100 papers (Total: 7500)\n",
      "✅ Retrieved 100 papers (Total: 7600)\n",
      "✅ Retrieved 100 papers (Total: 7700)\n",
      "✅ Retrieved 100 papers (Total: 7800)\n",
      "✅ Retrieved 100 papers (Total: 7900)\n",
      "✅ Retrieved 100 papers (Total: 8000)\n",
      "✅ Retrieved 100 papers (Total: 8100)\n",
      "✅ Retrieved 100 papers (Total: 8200)\n",
      "✅ Retrieved 100 papers (Total: 8300)\n",
      "✅ Retrieved 100 papers (Total: 8400)\n",
      "✅ Retrieved 100 papers (Total: 8500)\n",
      "✅ Retrieved 100 papers (Total: 8600)\n",
      "✅ Retrieved 100 papers (Total: 8700)\n",
      "✅ Retrieved 100 papers (Total: 8800)\n",
      "✅ Retrieved 100 papers (Total: 8900)\n",
      "✅ Retrieved 100 papers (Total: 9000)\n",
      "✅ Retrieved 100 papers (Total: 9100)\n",
      "✅ Retrieved 100 papers (Total: 9200)\n",
      "✅ Retrieved 100 papers (Total: 9300)\n",
      "✅ Retrieved 100 papers (Total: 9400)\n",
      "✅ Retrieved 100 papers (Total: 9500)\n",
      "✅ Retrieved 100 papers (Total: 9600)\n",
      "✅ Retrieved 100 papers (Total: 9700)\n",
      "✅ Retrieved 100 papers (Total: 9800)\n",
      "✅ Retrieved 100 papers (Total: 9900)\n",
      "✅ Retrieved 100 papers (Total: 10000)\n",
      "✅ Retrieved 100 papers (Total: 10100)\n",
      "✅ Retrieved 100 papers (Total: 10200)\n",
      "✅ Retrieved 100 papers (Total: 10300)\n",
      "✅ Retrieved 100 papers (Total: 10400)\n",
      "✅ Retrieved 100 papers (Total: 10500)\n",
      "✅ Retrieved 100 papers (Total: 10600)\n",
      "✅ Retrieved 100 papers (Total: 10700)\n",
      "✅ Retrieved 100 papers (Total: 10800)\n",
      "✅ Retrieved 100 papers (Total: 10900)\n",
      "✅ Retrieved 100 papers (Total: 11000)\n",
      "✅ Retrieved 100 papers (Total: 11100)\n",
      "✅ Retrieved 100 papers (Total: 11200)\n",
      "✅ Retrieved 100 papers (Total: 11300)\n",
      "✅ Retrieved 100 papers (Total: 11400)\n",
      "✅ Retrieved 100 papers (Total: 11500)\n",
      "✅ Retrieved 100 papers (Total: 11600)\n",
      "✅ Retrieved 100 papers (Total: 11700)\n",
      "✅ Retrieved 100 papers (Total: 11800)\n",
      "✅ Retrieved 100 papers (Total: 11900)\n",
      "✅ Retrieved 100 papers (Total: 12000)\n",
      "✅ Retrieved 100 papers (Total: 12100)\n",
      "✅ Retrieved 100 papers (Total: 12200)\n",
      "✅ Retrieved 100 papers (Total: 12300)\n",
      "✅ Retrieved 100 papers (Total: 12400)\n",
      "✅ Retrieved 100 papers (Total: 12500)\n",
      "✅ Retrieved 100 papers (Total: 12600)\n",
      "✅ Retrieved 100 papers (Total: 12700)\n",
      "✅ Retrieved 100 papers (Total: 12800)\n",
      "✅ Retrieved 100 papers (Total: 12900)\n",
      "✅ Retrieved 100 papers (Total: 13000)\n",
      "✅ Retrieved 100 papers (Total: 13100)\n",
      "✅ Retrieved 100 papers (Total: 13200)\n",
      "✅ Retrieved 100 papers (Total: 13300)\n",
      "✅ Retrieved 100 papers (Total: 13400)\n",
      "✅ Retrieved 100 papers (Total: 13500)\n",
      "✅ Retrieved 100 papers (Total: 13600)\n",
      "✅ Retrieved 100 papers (Total: 13700)\n",
      "✅ Retrieved 100 papers (Total: 13800)\n",
      "✅ Retrieved 100 papers (Total: 13900)\n",
      "✅ Retrieved 100 papers (Total: 14000)\n",
      "✅ Retrieved 100 papers (Total: 14100)\n",
      "✅ Retrieved 100 papers (Total: 14200)\n",
      "✅ Retrieved 100 papers (Total: 14300)\n",
      "✅ Retrieved 100 papers (Total: 14400)\n",
      "✅ Retrieved 100 papers (Total: 14500)\n",
      "✅ Retrieved 100 papers (Total: 14600)\n",
      "✅ Retrieved 100 papers (Total: 14700)\n",
      "✅ Retrieved 100 papers (Total: 14800)\n",
      "✅ Retrieved 100 papers (Total: 14900)\n",
      "✅ Retrieved 100 papers (Total: 15000)\n",
      "✅ Retrieved 100 papers (Total: 15100)\n",
      "✅ Retrieved 100 papers (Total: 15200)\n",
      "✅ Retrieved 100 papers (Total: 15300)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 32\u001b[0m\n\u001b[1;32m     25\u001b[0m search \u001b[38;5;241m=\u001b[39m arxiv\u001b[38;5;241m.\u001b[39mSearch(\n\u001b[1;32m     26\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m     27\u001b[0m     max_results\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     28\u001b[0m     sort_by\u001b[38;5;241m=\u001b[39marxiv\u001b[38;5;241m.\u001b[39mSortCriterion\u001b[38;5;241m.\u001b[39mSubmittedDate\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m batch_papers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search\u001b[38;5;241m.\u001b[39mresults():\n\u001b[1;32m     33\u001b[0m     batch_papers\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mtitle,\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39msummary,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mentry_id\n\u001b[1;32m     39\u001b[0m     })\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# If no new papers are found, stop fetching\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Emerging_Research_Trends_Clustering/ertc/lib/python3.9/site-packages/arxiv/__init__.py:600\u001b[0m, in \u001b[0;36mClient._results\u001b[0;34m(self, search, offset)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    599\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_url(search, offset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_size)\n\u001b[0;32m--> 600\u001b[0m feed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Emerging_Research_Trends_Clustering/ertc/lib/python3.9/site-packages/arxiv/__init__.py:626\u001b[0m, in \u001b[0;36mClient._parse_feed\u001b[0;34m(self, url, first_page, _try_index)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03mFetches the specified URL and parses it with feedparser.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mIf a request fails or is unexpectedly empty, retries the request up to\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m`self.num_retries` times.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_try_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    628\u001b[0m     HTTPError,\n\u001b[1;32m    629\u001b[0m     UnexpectedEmptyPageError,\n\u001b[1;32m    630\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _try_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_retries:\n",
      "File \u001b[0;32m~/Desktop/Emerging_Research_Trends_Clustering/ertc/lib/python3.9/site-packages/arxiv/__init__.py:656\u001b[0m, in \u001b[0;36mClient.__try_parse_feed\u001b[0;34m(self, url, first_page, try_index)\u001b[0m\n\u001b[1;32m    654\u001b[0m         to_sleep \u001b[38;5;241m=\u001b[39m (required \u001b[38;5;241m-\u001b[39m since_last_request)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m    655\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_sleep)\n\u001b[0;32m--> 656\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_sleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequesting page (first: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m, try: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, first_page, try_index, url)\n\u001b[1;32m    660\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv.py/2.1.3\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the AI research query\n",
    "query = \"cat:cs.AI AND (artificial intelligence OR deep learning OR machine learning OR neural networks OR supervised learning OR unsupervised learning OR semi-supervised learning OR reinforcement learning OR self-supervised learning OR few-shot learning OR zero-shot learning OR contrastive learning OR multi-modal learning OR transfer learning OR generative models OR autoencoders OR diffusion models OR attention mechanisms OR representation learning OR adversarial learning OR optimization algorithms OR neurosymbolic AI OR causality in AI OR graph-based learning OR computational neuroscience OR meta-learning OR scalable AI OR knowledge distillation OR neural compression OR transformers OR large language models OR prompt engineering OR in-context learning OR LLMs OR GPT OR BERT OR T5 OR vision transformers OR graph neural networks OR diffusion models OR GANs OR VAE OR AI fairness OR explainable AI OR federated learning OR privacy-preserving AI OR quantum machine learning OR neuromorphic computing OR AI ethics OR human-AI collaboration OR robotics and AI OR autonomous vehicles OR AI for healthcare OR AI for finance OR AI for scientific discovery OR AI for cybersecurity OR adversarial robustness OR AI for business forecasting OR AI for industrial automation OR AI for climate change OR AI for social good OR trustworthy AI OR causal inference OR explainable reinforcement learning) AND submittedDate:[20150101 TO 20251231]\"\n",
    "\n",
    "# Initialize result storage\n",
    "papers = []\n",
    "batch_size = 100  # Maximum allowed per request\n",
    "file_path = \"all_ai_research_papers.csv\"\n",
    "\n",
    "# Function to save results progressively\n",
    "def save_to_csv(data, file_path, first_save=False):\n",
    "    df = pd.DataFrame(data)\n",
    "    mode = 'w' if first_save else 'a'  # 'w' creates a new file, 'a' appends data\n",
    "    header = first_save  # Only include headers if creating a new file\n",
    "    df.to_csv(file_path, mode=mode, header=header, index=False)\n",
    "\n",
    "# Fetch papers continuously until no more are available\n",
    "first_save = True  # Flag to track first save operation\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=batch_size,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "\n",
    "        batch_papers = []\n",
    "        for result in search.results():\n",
    "            batch_papers.append({\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"published_date\": result.published.date(),\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"arxiv_url\": result.entry_id\n",
    "            })\n",
    "\n",
    "        # If no new papers are found, stop fetching\n",
    "        if not batch_papers:\n",
    "            print(f\"✅ No more results found. Fetching complete!\")\n",
    "            break\n",
    "\n",
    "        # Save batch to CSV\n",
    "        save_to_csv(batch_papers, file_path, first_save)\n",
    "        first_save = False  # After first save, switch to append mode\n",
    "\n",
    "        # Append batch to in-memory list\n",
    "        papers.extend(batch_papers)\n",
    "        print(f\"✅ Retrieved {len(batch_papers)} papers (Total: {len(papers)})\")\n",
    "\n",
    "        # Prevent hitting API rate limits\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {e}\")\n",
    "        print(\"🔄 Retrying after a short delay...\")\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"\\n✅ Completed! Collected {len(papers)} AI research papers and saved them to '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total AI-related papers saved: 58584\n",
      "Filtered data saved as 'filtered_arxiv_AI_papers.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define AI-related keywords\n",
    "keywords = [\n",
    "    \"artificial intelligence\", \"deep learning\", \"machine learning\", \"neural networks\",\n",
    "    \"supervised learning\", \"unsupervised learning\", \"semi-supervised learning\",\n",
    "    \"reinforcement learning\", \"self-supervised learning\", \"few-shot learning\",\n",
    "    \"zero-shot learning\", \"contrastive learning\", \"multi-modal learning\",\n",
    "    \"transfer learning\", \"generative models\", \"autoencoders\", \"diffusion models\",\n",
    "    \"attention mechanisms\", \"representation learning\", \"adversarial learning\",\n",
    "    \"optimization algorithms\", \"neurosymbolic AI\", \"causality in AI\",\n",
    "    \"graph-based learning\", \"computational neuroscience\", \"meta-learning\",\n",
    "    \"scalable AI\", \"knowledge distillation\", \"neural compression\", \"transformers\",\n",
    "    \"large language models\", \"prompt engineering\", \"in-context learning\",\n",
    "    \"LLMs\", \"GPT\", \"BERT\", \"T5\", \"vision transformers\", \"graph neural networks\",\n",
    "    \"GANs\", \"VAE\", \"AI fairness\", \"explainable AI\", \"federated learning\",\n",
    "    \"privacy-preserving AI\", \"quantum machine learning\", \"neuromorphic computing\",\n",
    "    \"AI ethics\", \"human-AI collaboration\", \"robotics and AI\", \"autonomous vehicles\",\n",
    "    \"AI for healthcare\", \"AI for finance\", \"AI for scientific discovery\",\n",
    "    \"AI for cybersecurity\", \"adversarial robustness\", \"AI for business forecasting\",\n",
    "    \"AI for industrial automation\", \"AI for climate change\", \"AI for social good\",\n",
    "    \"trustworthy AI\", \"causal inference\", \"explainable reinforcement learning\"\n",
    "]\n",
    "\n",
    "# Initialize list to store filtered results\n",
    "filtered_papers = []\n",
    "\n",
    "main_directory = os.path.join('/', *os.getcwd().split('/')[:-1])\n",
    "data_folder = os.path.join(main_directory, 'data')\n",
    "\n",
    "# Open the JSON file and process line by line\n",
    "with open(f\"{data_folder}/arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # Parse JSON line\n",
    "        paper = json.loads(line)\n",
    "\n",
    "        # Extract necessary fields\n",
    "        categories = paper.get(\"categories\", \"\")\n",
    "        abstract = paper.get(\"abstract\", \"\").lower()\n",
    "        title = paper.get(\"title\", \"\")\n",
    "        authors = paper.get(\"authors\", \"\")\n",
    "        doi = paper.get(\"doi\", \"\")\n",
    "        versions = paper.get(\"versions\", [])\n",
    "\n",
    "        # Extract first submission date\n",
    "        submitted_date = None\n",
    "        if isinstance(versions, list) and len(versions) > 0:\n",
    "            submitted_date = versions[0].get(\"created\", None)\n",
    "\n",
    "        # Convert to datetime format and extract year\n",
    "        if submitted_date:\n",
    "            submitted_year = pd.to_datetime(submitted_date, errors=\"coerce\").year\n",
    "        else:\n",
    "            continue  # Skip papers with no date\n",
    "\n",
    "        # Apply filters: Category, Abstract, and Date\n",
    "        if \"cs.AI\" in categories and any(keyword in abstract for keyword in keywords) and 2015 <= submitted_year <= 2025:\n",
    "            filtered_papers.append({\n",
    "                \"Title\": title,\n",
    "                \"Abstract\": abstract,\n",
    "                \"Year\": submitted_year,\n",
    "                \"Authors\": authors,\n",
    "                \"DOI\": doi\n",
    "            })\n",
    "\n",
    "        # Optional: Stop after processing a certain number of papers (for testing)\n",
    "        # if len(filtered_papers) >= 50000: break\n",
    "\n",
    "# Convert to DataFrame\n",
    "filtered_df = pd.DataFrame(filtered_papers)\n",
    "\n",
    "# Save to CSV\n",
    "filtered_df.to_csv(\"filtered_arxiv_AI_papers.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total AI-related papers saved: {len(filtered_df)}\")\n",
    "print(\"Filtered data saved as 'filtered_arxiv_AI_papers.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Information-theoretic and Algorithmic Appr...</td>\n",
       "      <td>we survey concepts at the frontier of resear...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Nicolas Gauvrit, Hector Zenil, Jesper Tegn\\'er</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Statistical-mechanical analysis of pre-trainin...</td>\n",
       "      <td>in this paper, we present a statistical-mech...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Masayuki Ohzeki</td>\n",
       "      <td>10.7566/JPSJ.84.034003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Injury risk prediction for traffic accidents i...</td>\n",
       "      <td>this study describes the experimental applic...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Christian S. Perone</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delving Deep into Rectifiers: Surpassing Human...</td>\n",
       "      <td>rectified activation units (rectifiers) are ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Quantum Production Model</td>\n",
       "      <td>the production system is a theoretical model...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Lu\\'is Tarrataca and Andreas Wichert</td>\n",
       "      <td>10.1007/s11128-011-0241-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  The Information-theoretic and Algorithmic Appr...   \n",
       "1  Statistical-mechanical analysis of pre-trainin...   \n",
       "2  Injury risk prediction for traffic accidents i...   \n",
       "3  Delving Deep into Rectifiers: Surpassing Human...   \n",
       "4                         A Quantum Production Model   \n",
       "\n",
       "                                            Abstract  Year  \\\n",
       "0    we survey concepts at the frontier of resear...  2015   \n",
       "1    in this paper, we present a statistical-mech...  2015   \n",
       "2    this study describes the experimental applic...  2015   \n",
       "3    rectified activation units (rectifiers) are ...  2015   \n",
       "4    the production system is a theoretical model...  2015   \n",
       "\n",
       "                                             Authors  \\\n",
       "0     Nicolas Gauvrit, Hector Zenil, Jesper Tegn\\'er   \n",
       "1                                    Masayuki Ohzeki   \n",
       "2                                Christian S. Perone   \n",
       "3  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun   \n",
       "4               Lu\\'is Tarrataca and Andreas Wichert   \n",
       "\n",
       "                         DOI  \n",
       "0                       None  \n",
       "1     10.7566/JPSJ.84.034003  \n",
       "2                       None  \n",
       "3                       None  \n",
       "4  10.1007/s11128-011-0241-2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()  # Display the first few rows of the filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total AI-related papers saved: 110272\n",
      "Filtered data saved as 'filtered_arxiv_AI_papers.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define AI-related keywords\n",
    "keywords = [\n",
    "    \"artificial intelligence\", \"deep learning\", \"machine learning\", \"neural networks\",\n",
    "    \"supervised learning\", \"unsupervised learning\", \"semi-supervised learning\",\n",
    "    \"reinforcement learning\", \"self-supervised learning\", \"few-shot learning\",\n",
    "    \"zero-shot learning\", \"contrastive learning\", \"multi-modal learning\",\n",
    "    \"transfer learning\", \"generative models\", \"autoencoders\", \"diffusion models\",\n",
    "    \"attention mechanisms\", \"representation learning\", \"adversarial learning\",\n",
    "    \"optimization algorithms\", \"neurosymbolic AI\", \"causality in AI\",\n",
    "    \"graph-based learning\", \"computational neuroscience\", \"meta-learning\",\n",
    "    \"scalable AI\", \"knowledge distillation\", \"neural compression\", \"transformers\",\n",
    "    \"large language models\", \"prompt engineering\", \"in-context learning\",\n",
    "    \"LLMs\", \"GPT\", \"BERT\", \"T5\", \"vision transformers\", \"graph neural networks\",\n",
    "    \"GANs\", \"VAE\", \"AI fairness\", \"explainable AI\", \"federated learning\",\n",
    "    \"privacy-preserving AI\", \"quantum machine learning\", \"neuromorphic computing\",\n",
    "    \"AI ethics\", \"human-AI collaboration\", \"robotics and AI\", \"autonomous vehicles\",\n",
    "    \"AI for healthcare\", \"AI for finance\", \"AI for scientific discovery\",\n",
    "    \"AI for cybersecurity\", \"adversarial robustness\", \"AI for business forecasting\",\n",
    "    \"AI for industrial automation\", \"AI for climate change\", \"AI for social good\",\n",
    "    \"trustworthy AI\", \"causal inference\", \"explainable reinforcement learning\"\n",
    "]\n",
    "\n",
    "# Initialize list to store filtered results\n",
    "filtered_papers = []\n",
    "\n",
    "main_directory = os.path.join('/', *os.getcwd().split('/')[:-1])\n",
    "data_folder = os.path.join(main_directory, 'data')\n",
    "\n",
    "# Open the JSON file and process line by line\n",
    "with open(f\"{data_folder}/arxiv-metadata-oai-snapshot.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # Parse JSON line\n",
    "        paper = json.loads(line)\n",
    "\n",
    "        # Extract necessary fields\n",
    "        categories = paper.get(\"categories\", \"\")\n",
    "        abstract = paper.get(\"abstract\", \"\").lower()\n",
    "        title = paper.get(\"title\", \"\")\n",
    "        authors = paper.get(\"authors\", \"\")\n",
    "        doi = paper.get(\"doi\", \"\")\n",
    "        versions = paper.get(\"versions\", [])\n",
    "\n",
    "        # Extract first submission date\n",
    "        submitted_date = None\n",
    "        if isinstance(versions, list) and len(versions) > 0:\n",
    "            submitted_date = versions[0].get(\"created\", None)\n",
    "\n",
    "        # Convert to datetime format and extract year\n",
    "        if submitted_date:\n",
    "            submitted_year = pd.to_datetime(submitted_date, errors=\"coerce\").year\n",
    "        else:\n",
    "            continue  # Skip papers with no date\n",
    "\n",
    "        # Apply filters: Category, Abstract, and Date\n",
    "        if \"cs.AI\" in categories and 2015 <= submitted_year <= 2025:\n",
    "            filtered_papers.append({\n",
    "                \"Title\": title,\n",
    "                \"Abstract\": abstract,\n",
    "                \"Year\": submitted_year,\n",
    "                \"Authors\": authors,\n",
    "                \"DOI\": doi\n",
    "            })\n",
    "\n",
    "        # Optional: Stop after processing a certain number of papers (for testing)\n",
    "        # if len(filtered_papers) >= 50000: break\n",
    "\n",
    "# Convert to DataFrame\n",
    "filtered_df = pd.DataFrame(filtered_papers)\n",
    "\n",
    "# Save to CSV\n",
    "filtered_df.to_csv(\"all_arxiv_AI_papers.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total AI-related papers saved: {len(filtered_df)}\")\n",
    "print(\"Filtered data saved as 'filtered_arxiv_AI_papers.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ultimate Intelligence Part I: Physical Complet...</td>\n",
       "      <td>we propose that solomonoff induction is comp...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Eray \\\"Ozkural</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hostile Intent Identification by Movement Patt...</td>\n",
       "      <td>in the recent years, the problem of identify...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Souham Biswas, Manisha J. Nene</td>\n",
       "      <td>10.13140/2.1.4429.7281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Novel Design of a Parallel Machine Learnt Ge...</td>\n",
       "      <td>the generational garbage collection involves...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Vasanthakumar Soundararajan</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Constraint-based sequence mining using constra...</td>\n",
       "      <td>the goal of constraint-based sequence mining...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Benjamin Negrevergne and Tias Guns</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>On the Relationship between Sum-Product Networ...</td>\n",
       "      <td>in this paper, we establish some theoretical...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Han Zhao, Mazen Melibari and Pascal Poupart</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Ultimate Intelligence Part I: Physical Complet...   \n",
       "1  Hostile Intent Identification by Movement Patt...   \n",
       "2  A Novel Design of a Parallel Machine Learnt Ge...   \n",
       "3  Constraint-based sequence mining using constra...   \n",
       "4  On the Relationship between Sum-Product Networ...   \n",
       "\n",
       "                                            Abstract  Year  \\\n",
       "0    we propose that solomonoff induction is comp...  2015   \n",
       "1    in the recent years, the problem of identify...  2015   \n",
       "2    the generational garbage collection involves...  2015   \n",
       "3    the goal of constraint-based sequence mining...  2015   \n",
       "4    in this paper, we establish some theoretical...  2015   \n",
       "\n",
       "                                       Authors                     DOI  \n",
       "0                               Eray \\\"Ozkural                    None  \n",
       "1               Souham Biswas, Manisha J. Nene  10.13140/2.1.4429.7281  \n",
       "2                  Vasanthakumar Soundararajan                    None  \n",
       "3           Benjamin Negrevergne and Tias Guns                    None  \n",
       "4  Han Zhao, Mazen Melibari and Pascal Poupart                    None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()  # Display the first few rows of the filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ertc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
